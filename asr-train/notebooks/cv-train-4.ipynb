{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is to prepare the `training-report.pdf` for task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_DIR = Path.home() / \"work/htx-xdata\"  # TODO change this to the path of your repo\n",
    "TASK_DIR = PROJECT_DIR / \"asr-train\"\n",
    "src_dir = TASK_DIR / \"src\"\n",
    "\n",
    "\n",
    "if src_dir.as_posix() not in sys.path:\n",
    "    sys.path.insert(0, src_dir.as_posix())\n",
    "# NOTE: You may also want to add `\"python.analysis.extraPaths\": [\"./asr-train/src\"]` to your VSCode workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import re\n",
    "import shlex\n",
    "from functools import partial\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "from subprocess import check_output\n",
    "from typing import List, Tuple\n",
    "\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from app.config import (  # pth_df_wer_test,\n",
    "    best_checkpoint_dir,\n",
    "    ds_dev_dir,\n",
    "    ds_test_dir,\n",
    "    ds_train_dir,\n",
    "    ds_val_dir,\n",
    "    final_model_dir,\n",
    "    pth_df_wer_dev,\n",
    "    pth_valid_dev_raw,\n",
    "    pth_valid_test_raw,\n",
    "    pth_valid_train_raw,\n",
    "    sampling_rate,\n",
    "    valid_dev_raw_dir,\n",
    "    valid_test_raw_dir,\n",
    "    valid_train_raw_dir,\n",
    ")\n",
    "from app.model import ASRModel\n",
    "from datasets import Dataset, load_dataset\n",
    "from pydub import AudioSegment\n",
    "from sklearn.model_selection import train_test_split\n",
    "from utils_ds import (\n",
    "    array_to_audio,\n",
    "    backup_file,\n",
    "    disp_audio,\n",
    "    get_df_valid_dev,\n",
    "    get_df_valid_test,\n",
    "    get_df_valid_train,\n",
    "    get_df_wer,\n",
    "    get_df_wer_dev,\n",
    "    get_ds_cur_chunk_dir,\n",
    "    get_ds_fingerprint_chunk,\n",
    "    load_ds_from_disk,\n",
    "    preprocess_text,\n",
    "    save_best_model,\n",
    ")\n",
    "from utils_train import CommonVoiceDataLoader, batch_predict, count_parameters\n",
    "\n",
    "pd.options.display.max_colwidth = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load dev dataset\n",
    "ds_dev = Dataset.load_from_disk(ds_dev_dir)\n",
    "\n",
    "## Load dev set predictions dataframe (or predict if not available)\n",
    "\n",
    "df_dev = get_df_valid_dev()\n",
    "\n",
    "# case: No prediction yet -> predict\n",
    "if \"generated_text_finetuned\" not in df_dev:\n",
    "\n",
    "    asr = ASRModel(pth_model=final_model_dir)\n",
    "    dev_cv_dataloader = CommonVoiceDataLoader(ds_dev, asr.processor, batch_size=32)\n",
    "    dev_loader = dev_cv_dataloader.get_dataloader(shuffle=False)\n",
    "\n",
    "    backup_file(pth_valid_dev_raw)\n",
    "    df_dev_raw = pd.read_csv(pth_valid_dev_raw)\n",
    "\n",
    "    preds, labels, filenames = batch_predict(dev_loader, asr.model, asr.processor, asr.device)\n",
    "    df_wip = pd.DataFrame(\n",
    "        {\n",
    "            \"filename\": filenames,\n",
    "            \"generated_text_finetuned\": preds,\n",
    "            \"label\": labels,\n",
    "        }\n",
    "    )\n",
    "    df_dev_raw_w_preds = df_dev_raw.merge(df_wip, on=\"filename\", how=\"left\")\n",
    "\n",
    "    # Write\n",
    "    df_dev_raw_w_preds.to_csv(pth_valid_dev_raw, index=False)\n",
    "\n",
    "    # reload\n",
    "    df_dev = get_df_valid_dev()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute WER for dev set\n",
    "# [result] WER: (original, finetuned) = (0.11030, 0.14030)\n",
    "\n",
    "RUN = False\n",
    "if RUN:\n",
    "    wer_metric = evaluate.load(\"wer\")\n",
    "    wer = wer_metric.compute(predictions=df_dev[\"generated_text\"], references=df_dev[\"label\"])\n",
    "    wer_finetuned = wer_metric.compute(predictions=df_dev[\"generated_text_finetuned\"], references=df_dev[\"label\"])\n",
    "    print(f\"WER: (original, finetuned) = ({wer:.5f}, {wer_finetuned:.5f})\")\n",
    "del RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the WER for both the original and finetuned models\n",
    "df_wer_dev = get_df_wer_dev(pth_df_wer_dev, df_dev, ds_dev)\n",
    "df_wer_dev.rename(\n",
    "    columns={\n",
    "        \"generated_text\": \"pred_old\",\n",
    "        \"generated_text_finetuned\": \"pred_new\",\n",
    "        \"wer\": \"wer_old\",\n",
    "        \"wer_finetuned\": \"wer_new\",\n",
    "    },\n",
    "    inplace=True,\n",
    ")\n",
    "cols = [\"filename\", \"stats\", \"text\", \"label\", \"pred_old\", \"pred_new\", \"wer_old\", \"wer_new\"]\n",
    "df_wer_dev = df_wer_dev.reindex(columns=cols)\n",
    "df_wer_dev[\"wer_diff\"] = df_wer_dev[\"wer_new\"] - df_wer_dev[\"wer_old\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_same_wer = df_wer_dev[\"wer_new\"] == df_wer_dev[\"wer_old\"]\n",
    "cond_new_better = df_wer_dev[\"wer_new\"] < df_wer_dev[\"wer_old\"]\n",
    "cond_new_worse = df_wer_dev[\"wer_new\"] > df_wer_dev[\"wer_old\"]\n",
    "\n",
    "cond_old_0wer = df_wer_dev[\"wer_old\"] == 0\n",
    "\n",
    "n_same_0wer = (cond_same_wer & cond_old_0wer).sum()\n",
    "n_same_wer_nonzero = (cond_same_wer & ~cond_old_0wer).sum()\n",
    "n_new_better = cond_new_better.sum()\n",
    "n_new_worse = cond_new_worse.sum()\n",
    "\n",
    "pct_same_0wer = n_same_0wer / df_wer_dev.shape[0]\n",
    "pct_same_wer_nonzero = n_same_wer_nonzero / df_wer_dev.shape[0]\n",
    "pct_new_better = n_new_better / df_wer_dev.shape[0]\n",
    "pct_new_worse = n_new_worse / df_wer_dev.shape[0]\n",
    "\n",
    "\n",
    "print(\n",
    "    f\"Both fully-correct (0 WER): {n_same_0wer:12d} ({pct_same_0wer:06.2%})\"\n",
    "    \"\\n\"\n",
    "    f\"Both same non-zero WER    : {n_same_wer_nonzero:12d} ({pct_same_wer_nonzero:06.2%})\"\n",
    "    \"\\n\"\n",
    "    f\"New model better          : {n_new_better:12d} ({pct_new_better:06.2%})\"\n",
    "    \"\\n\"\n",
    "    f\"New model worse           : {n_new_worse:12d} ({pct_new_worse:06.2%})\"\n",
    ")\n",
    "\n",
    "# # [results]\n",
    "# Both fully-correct (0 WER):         1713 (42.04%)\n",
    "# Both same non-zero WER    :          941 (23.09%)\n",
    "# New model better          :          376 (09.23%)\n",
    "# New model worse           :         1045 (25.64%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [New model worse] Analyze largest WER difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_debug = df_wer_dev.sort_values(\"wer_diff\", ascending=False).query(\"wer_diff > 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_row = 7\n",
    "row = df_debug.iloc[i_row]\n",
    "display(row.to_frame().T)\n",
    "disp_audio(df_debug.iloc[i_row][\"filename\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.config import pth_analyze_largest_wer_diff\n",
    "\n",
    "df_analyze = pd.read_csv(pth_analyze_largest_wer_diff)\n",
    "display(df_analyze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for x in df_debug['filename'].iloc[:20]:\n",
    "#     print(f\"1. `{x}` ->\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [New model better] Analyze largest WER difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_debug = df_wer_dev.sort_values(\"wer_diff\", ascending=True).query(\"wer_diff < 0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. `cv-valid-dev/sample-001346.mp3` -> (US; US; male)\n",
    "1. `cv-valid-dev/sample-003047.mp3` -> (US; US; male). 20+s audio had additional conversation which was picked up fairly well by the original model, but is not the provided ground truth.\n",
    "1. `cv-valid-dev/sample-002782.mp3` -> (US; US; male). Model was able to predict correct locations of `'` and the word \"invasion\". 20+s audio with additional conversation.\n",
    "1. `cv-valid-dev/sample-001352.mp3` -> (non-US; UK; male). Model is able to predict \"I'm\" instead of \"I AM\" by the original model.\n",
    "1. `cv-valid-dev/sample-001015.mp3` -> (non-US; UK; male). Model was able to get \"NOT\" and \"SO\" correct over the original model.\n",
    "1. `cv-valid-dev/sample-000606.mp3` -> (non-US; unk; male). Model was able to predict \"I'm\" and correct words.\n",
    "1. `cv-valid-dev/sample-000723.mp3` -> (non-US; unk; male). Model was able to predict \"I'll\".\n",
    "1. `cv-valid-dev/sample-002313.mp3` -> (non-US; unk; male). Model was able to predict \"LET'S\" (original predicted \"LAT'S\").\n",
    "1. `cv-valid-dev/sample-004001.mp3` -> (non-US; UK; male). Proper English audio. Model was able to predict correctly.\n",
    "1. `cv-valid-dev/sample-003659.mp3` -> (non-US; unk; male). Muffled audio, heavily accented. Model was able to predict correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_row = 9\n",
    "row = df_debug.iloc[i_row]\n",
    "display(row.to_frame().T)\n",
    "disp_audio(df_debug.iloc[i_row][\"filename\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [KIV] Manually analyzing random samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Misc explorations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_accent_classifier():\n",
    "    \"\"\"Just a quick exploration to do accent classification\"\"\"\n",
    "    from utils_experimental import AccentClassifier\n",
    "\n",
    "    clf = AccentClassifier(\"pretrained_models/accent-id-commonaccent_xlsr-en-english\")\n",
    "    filenames = df_dev.sample(10, random_state=42)[\"filename\"]\n",
    "    df_eda = df_dev.query(\"filename in @filenames\").copy()\n",
    "    pths = df_eda[\"filename\"].apply(lambda x: valid_dev_raw_dir.parent / x).tolist()\n",
    "    preds = clf.batch_predict_accent(pths)\n",
    "    df_eda[\"accent_pred\"] = preds\n",
    "    display(df_eda)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "htx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
